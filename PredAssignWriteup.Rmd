---
title: "Prediction Assignment Writeup"
author: "Olmedo Alonso Madrigales"
date: "7/17/2021"
output: html_document
---

# Background

In this course project I have been asked to predict the manner in which some enthusiastic people, who do exercises, take measurements about themselves using devices such as Jawbone Up, Nike FuelBand, and Fitbit.  It is now possible to collect large amounts of data about personal activity relatively inexpensively. 

These type of devices are part of the quantified self movement where these folks take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, my primary goal is to use data from these accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

# Data

The data for this project comes from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har thanks to the generous contribution of Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. who wrote the Qualitative Activity Recognition of Weight Lifting Exercises document used in the Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013

Read more: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har#weight_lifting_exercises

The data was divided in two groups: training and testing, provided both in the following links:

#### The training data 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

#### The testing data
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Data Processing Set Up

Here I provide the preparation steps to process the data from setting the directory and calling all the libraries I will use in this project. 

```{r libraries, echo=TRUE, results='hide', message=FALSE, warning=FALSE}

setwd("C:/Users/Olmedo/Desktop/Coursera/datasciencecoursera/MachineLearning")
library(caret)
library(stats)
library(kernlab)
library(rattle)
library(knitr)
library(corrplot)
library(kableExtra)
```

### Data Gathering

I access the links that were provided with the data using the next codes.  I also check the dimension of the dataframe.

```{r gathering, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
training <- file.path("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
datatrain <- read.table(training, header = TRUE, sep = ",")
testing <- file.path("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
datatest <- read.table(testing, header = TRUE, sep = ",")
dim(datatrain)
dim(datatest)
```

With these info I also review the data using the function str(datatrain) and str(datatest) to get acquainted with them in order to clean it and downsize it to what I need for the operation.

### Data Cleaning

First I eliminate the columns with NAs values.

```{r cleaning, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
datatrain <- datatrain[, colSums(is.na(datatrain))==0]
datatest <- datatest[, colSums(is.na(datatest))==0]
dim(datatrain)
dim(datatest)
```

Then I eliminate the first 7 columns that are unnecessary

```{r t&t, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
datatrain <- datatrain[, -c(1:7)]
datatest <- datatest[, -c(1:7)]
dim(datatrain)
dim(datatest)
```

### Data Partition

Now with the dataframes already clean, I advance to split the training data in two subsets training and testing leaving alone the original test data to the final checks.

```{r predict, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
set.seed(1234)
training_set <- createDataPartition(datatrain$classe, p=0.7, list = FALSE)
training_data <- datatrain[training_set, ]
testing_data <- datatrain[-training_set, ]
dim(training_data)
dim(testing_data)
```

### Further Subset Data Cleaning

It is important to remove all near zero variance variables so the data is more compact.

```{r cleaning1, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
nzv <- nearZeroVar(training_data)
training_data <- training_data[, -nzv]
testing_data <- testing_data[, -nzv]
dim(training_data)
dim(testing_data)
```

### Correlation Analisys

A correlation among variables is analyzed before proceeding to the modeling procedures.  The highly correlated variables are shown in dark colors in the graph beneath.

```{r correlation, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
cor_ana <- cor(training_data[,-53])
corrplot(cor_ana, order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

# Model Building

I will test a few popular models including: Decision Trees, Random Forest and Gradient Boosted Trees to then analyze which is more accurate for the exercise. I will set up control for the training set using 3 fold cross validation.


```{r validation, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
control <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
```

## Decision Tree

```{r tree, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
model_tree <- train(classe~., data=training_data, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(model_tree$finalModel)
```

Now lets validate this model with the Test Data (testing_data)

```{r prediction, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
pred_tree <- predict(model_tree, testing_data)
cmtree <- confusionMatrix(pred_tree, factor(testing_data$classe))
cmtree
```

## Random Forest

```{r RF, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
model_rf <- train(classe~., data=training_data, method="rf", trControl = control, tuneLength = 5)

pred_rf <- predict(model_rf, testing_data)
cmrf <- confusionMatrix(pred_rf, factor(testing_data$classe))
cmrf
```

## Gradient Boosted Trees
```{r GBM, echo=TRUE, message=FALSE, warning=FALSE}
model_gbm <- train(classe~., data=training_data, method="gbm", trControl = control, tuneLength = 5, verbose = F)

pred_gbm <- predict(model_gbm, testing_data)
cmgbm <- confusionMatrix(pred_gbm, factor(testing_data$classe))
cmgbm
```

# Results on Accuracy for the 3 Models

```{r Accuracy, results='asis', echo=TRUE, message=FALSE, warning=FALSE}
Accuracy_model_tree <- paste("Decision Tree - Accuracy =",
                  round(cmtree$overall['Accuracy'], 4))
print(Accuracy_model_tree)
Accuracy_RF <- paste("Random Forest - Accuracy =",
                  round(cmrf$overall['Accuracy'], 4))
print(Accuracy_RF)
Accuracy_GBM <- paste("Gradient Boosted Trees - Accuracy =",
                  round(cmgbm$overall['Accuracy'], 4))
print(Accuracy_GBM)
```

# Predictions on Test Set

Now the more fitted model is Random Forest, so letÂ´s make predictions based on this model.

```{r Predict Test, echo=TRUE, message=FALSE, warning=FALSE}
pred <- predict(model_rf, datatest)
print(pred)
```


